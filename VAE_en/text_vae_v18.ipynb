{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入需要的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Bidirectional, Dense, Embedding, Input, Lambda, LSTM, RepeatVector, TimeDistributed, Layer, Activation, Dropout\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers.advanced_activations import ELU\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from scipy import spatial\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import codecs\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载目录和文档\n",
    "首先，我们将设置主目录和一些有关文本特征的变量。我们将最大序列长度设置为25，将词汇表中的最大单词数设置为12000，我们将使用300维embeddings。最后，从csv加载文本。文本文件是Quora Kaggle挑战的训练文件，包含大约808000个句子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 784059 texts in train.csv\n"
     ]
    }
   ],
   "source": [
    "BASE_DIR = './data/'\n",
    "TRAIN_DATA_FILE = BASE_DIR + 'train.csv'#80+万条问句\n",
    "GLOVE_EMBEDDING = BASE_DIR + 'glove.6B.300d.txt'#单词->300维embedding\n",
    "VALIDATION_SPLIT = 0.2\n",
    "MAX_SEQUENCE_LENGTH = 25 #最大序列长度25\n",
    "MAX_NB_WORDS = 100000\n",
    "EMBEDDING_DIM = 300 #embedding维度300\n",
    "\n",
    "texts = [] #通过列表来存储句子\n",
    "with codecs.open(TRAIN_DATA_FILE, encoding='utf-8') as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    header = next(reader) #忽略标题行\n",
    "    for values in reader: #对每一行[3][4]是训练用的句子\n",
    "        if len(values[3].split()) <= MAX_SEQUENCE_LENGTH:\n",
    "            texts.append(values[3])\n",
    "        if len(values[4].split()) <= MAX_SEQUENCE_LENGTH:\n",
    "            texts.append(values[4])\n",
    "print('Found %s texts in train.csv' % len(texts)) #训练用句子个数\n",
    "n_sents = len(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文本预处理\n",
    "使用Keras的tokenizer和text_to_sequences函数预处理文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 91448 unique tokens\n",
      "Shape of data tensor: (784059, 25)\n",
      "NB_WORDS: 91449\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(MAX_NB_WORDS+1, oov_token='unk') #Tokenizer是一个用于向量化文本，或将文本转换为序列（即单词在字典中的下标构成的列表，从1算起）的类\n",
    "tokenizer.fit_on_texts(texts)\n",
    "print('Found %s unique tokens' % len(tokenizer.word_index))\n",
    "\n",
    "## **关键步骤** 若不能正常工作，丢弃OOV_Token\n",
    "tokenizer.word_index = {e:i for e,i in tokenizer.word_index.items() if i <= MAX_NB_WORDS} # <= 从1开始\n",
    "#tokenizer.word_index[tokenizer.oov_token] = MAX_NB_WORDS + 1\n",
    "word_index = tokenizer.word_index #word到index的字典\n",
    "index2word = {v: k for k, v in word_index.items()} #index到word的字典\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(texts)#序列的列表，列表中每个序列对应于一段输入文本\n",
    "data_1 = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH) #序列全部填充到25维，尾补0\n",
    "print('Shape of data tensor:', data_1.shape)\n",
    "NB_WORDS = (min(tokenizer.num_words, len(word_index))+1) #+1 for zero padding \n",
    "print('NB_WORDS:', NB_WORDS)\n",
    "\n",
    "data_val = data_1[775000:783000]\n",
    "data_train = data_1[:775000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embeddings\n",
    "使用预训练的Glove word embeddings。创建一个矩阵，在词汇表中为每个单词对应一个embedding，然后我们将这个矩阵作为权重传递给我们模型的embedding layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n",
      "Null word embeddings: 1\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open(GLOVE_EMBEDDING, encoding='utf8')\n",
    "\n",
    "#取出word及其对应的embeddings，存入字典embeddings_index\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "glove_embedding_matrix = np.zeros((NB_WORDS, EMBEDDING_DIM)) #申请0数组，\n",
    "for word, i in word_index.items():\n",
    "    if i < NB_WORDS+1: #+1 for 'unk' oov token\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            glove_embedding_matrix[i] = embedding_vector\n",
    "        else:\n",
    "            # 在embeddings索引中找不到的单词，将是unk的embeddings\n",
    "            glove_embedding_matrix[i] = embeddings_index.get('unk')\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(glove_embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(word_index['unk'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAE 模型\n",
    "模型基于seq2seq架构，包含双向LSTM编码器和LSTM解码器。\n",
    "通过 RepeatVector（max_len）函数，将每个时间步的潜在表示作为输入提供给解码器decoder。为了避免标签的独热码表示，我们使用tf.contrib.seq2seq.sequence_loss函数，它只需要单词索引作为标签（与embedding矩阵的输入相同）并在内部计算最终的softmax（所以 模型以具有线性激活的dense层结束）。 \n",
    "可选地，“sequence_loss”允许使用采样的softmax，这有助于处理大型词汇表（例如，具有50k字词汇），但在此没有使用。这里使用的解码器与文中实现的解码器不同; 不是将context vector作为解码器的初始状态和预测的单词作为输入，而是在每个时间步处输入潜在表示z作为输入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "(?, 25) (?, 25, 91449)\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 25)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 25, 300)      27434700    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 512)          1140736     embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 64)           32832       bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 64)           32832       bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 64)           0           dense_1[0][0]                    \n",
      "                                                                 dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_1 (RepeatVector)  (None, 25, 64)       0           lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 25, 256)      328704      repeat_vector_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 25, 91449)    23502393    lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "custom_variational_layer_1 (Cus [(None, 25), (None,  0           input_1[0][0]                    \n",
      "                                                                 dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 52,472,197\n",
      "Trainable params: 25,037,497\n",
      "Non-trainable params: 27,434,700\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "max_len = MAX_SEQUENCE_LENGTH\n",
    "emb_dim = EMBEDDING_DIM\n",
    "latent_dim = 64\n",
    "intermediate_dim = 256\n",
    "epsilon_std = 1.0\n",
    "kl_weight = 0.01\n",
    "num_sampled=500\n",
    "act = ELU()\n",
    "\n",
    "\n",
    "x = Input(shape=(max_len,)) #输入是按批量的25维向量(句子)\n",
    "x_embed = Embedding(NB_WORDS, emb_dim, weights=[glove_embedding_matrix], input_length=max_len, trainable=False)(x)\n",
    "h = Bidirectional(LSTM(intermediate_dim, return_sequences=False, recurrent_dropout=0.2), merge_mode='concat')(x_embed)\n",
    "#h = Bidirectional(LSTM(intermediate_dim, return_sequences=False), merge_mode='concat')(h)\n",
    "#h = Dropout(0.2)(h)\n",
    "#h = Dense(intermediate_dim, activation='linear')(h)\n",
    "#h = act(h)\n",
    "#h = Dropout(0.2)(h)\n",
    "z_mean = Dense(latent_dim)(h)\n",
    "z_log_var = Dense(latent_dim)(h)\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = K.random_normal(shape=(batch_size, latent_dim), mean=0.,\n",
    "                              stddev=epsilon_std)\n",
    "    return z_mean + K.exp(z_log_var / 2) * epsilon\n",
    "\n",
    "# note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "# 分别实例化这些层，以便以后重用\n",
    "repeated_context = RepeatVector(max_len)\n",
    "decoder_h = LSTM(intermediate_dim, return_sequences=True, recurrent_dropout=0.2)\n",
    "decoder_mean = Dense(NB_WORDS, activation='linear')#softmax is applied in the seq2seqloss by tf #TimeDistributed()\n",
    "h_decoded = decoder_h(repeated_context(z))\n",
    "x_decoded_mean = decoder_mean(h_decoded)\n",
    "\n",
    "\n",
    "# placeholder loss\n",
    "def zero_loss(y_true, y_pred):\n",
    "    return K.zeros_like(y_pred)\n",
    "\n",
    "#Sampled softmax\n",
    "#logits = tf.constant(np.random.randn(batch_size, max_len, NB_WORDS), tf.float32)\n",
    "#targets = tf.constant(np.random.randint(NB_WORDS, size=(batch_size, max_len)), tf.int32)\n",
    "#proj_w = tf.constant(np.random.randn(NB_WORDS, NB_WORDS), tf.float32)\n",
    "#proj_b = tf.constant(np.zeros(NB_WORDS), tf.float32)\n",
    "#\n",
    "#def _sampled_loss(labels, logits):\n",
    "#    labels = tf.cast(labels, tf.int64)\n",
    "#    labels = tf.reshape(labels, [-1, 1])\n",
    "#    logits = tf.cast(logits, tf.float32)\n",
    "#    return tf.cast(\n",
    "#                    tf.nn.sampled_softmax_loss(\n",
    "#                        proj_w,\n",
    "#                        proj_b,\n",
    "#                        labels,\n",
    "#                        logits,\n",
    "#                        num_sampled=num_sampled,\n",
    "#                        num_classes=NB_WORDS),\n",
    "#                    tf.float32)\n",
    "#softmax_loss_f = _sampled_loss\n",
    "\n",
    "\n",
    "# 用于计算VAE损失的自定义层\n",
    "class CustomVariationalLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.is_placeholder = True\n",
    "        super(CustomVariationalLayer, self).__init__(**kwargs)\n",
    "        self.target_weights = tf.constant(np.ones((batch_size, max_len)), tf.float32)\n",
    "\n",
    "    def vae_loss(self, x, x_decoded_mean):\n",
    "        #xent_loss = K.sum(metrics.categorical_crossentropy(x, x_decoded_mean), axis=-1)\n",
    "        labels = tf.cast(x, tf.int32)\n",
    "        xent_loss = K.sum(tf.contrib.seq2seq.sequence_loss(x_decoded_mean, labels, \n",
    "                                                     weights=self.target_weights,\n",
    "                                                     average_across_timesteps=False,\n",
    "                                                     average_across_batch=False), axis=-1)#,\n",
    "                                                     #softmax_loss_function=softmax_loss_f), axis=-1)#,\n",
    "        kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "        xent_loss = K.mean(xent_loss)\n",
    "        kl_loss = K.mean(kl_loss)\n",
    "        return K.mean(xent_loss + kl_weight * kl_loss)\n",
    "    \n",
    "    #编写一个call方法，来实现自定义层\n",
    "    def call(self, inputs):\n",
    "        x = inputs[0]\n",
    "        x_decoded_mean = inputs[1]\n",
    "        print(x.shape, x_decoded_mean.shape)\n",
    "        loss = self.vae_loss(x, x_decoded_mean)\n",
    "        self.add_loss(loss, inputs=inputs)\n",
    "        # we don't use this output, but it has to have the correct shape:\n",
    "        return K.ones_like(x)\n",
    "    \n",
    "def kl_loss(x, x_decoded_mean):\n",
    "    kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "    kl_loss = kl_weight * kl_loss\n",
    "    return kl_loss\n",
    "\n",
    "loss_layer = CustomVariationalLayer()([x, x_decoded_mean])\n",
    "vae = Model(x, [loss_layer])\n",
    "opt = Adam(lr=0.01) \n",
    "vae.compile(optimizer='adam', loss=[zero_loss], metrics=[kl_loss])\n",
    "vae.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型训练\n",
    "通过keras.fit()训练100epochs。对于验证数据，传递相同的数组两次，因为此模型的输入和标签相同。\n",
    "如果不使用“tf.contrib.seq2seq.sequence_loss”（或其他类似的函数），\n",
    "将必须作为标签传递单词的one-hot码高维度序列(batch_size，seq_len，vocab_size)消耗大量内存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_checkpoint(dir, model_name):\n",
    "    filepath = dir + '/' + model_name + \".h5\" \n",
    "    directory = os.path.dirname(filepath)\n",
    "    try:\n",
    "        os.stat(directory)\n",
    "    except:\n",
    "        os.mkdir(directory)\n",
    "    checkpointer = ModelCheckpoint(filepath=filepath, verbose=1, save_best_only=True)\n",
    "    return checkpointer\n",
    "\n",
    "checkpointer = create_model_checkpoint('models', 'vae_seq2seq_test_very_high_std')\n",
    "\n",
    "\n",
    "'''\n",
    "vae.fit(data_train, data_train,\n",
    "     shuffle=True,\n",
    "     epochs=100,\n",
    "     batch_size=batch_size,\n",
    "     validation_data=(data_val, data_val), callbacks=[checkpointer])\n",
    "'''\n",
    "#print(K.eval(vae.optimizer.lr))\n",
    "#K.set_value(vae.optimizer.lr, 0.01)\n",
    "\n",
    "#vae.save('models/vae_lstm.h5')\n",
    "vae.load_weights('models/vae_lstm.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 来自潜在空间的项目和样本句子\n",
    "构建一个编码器模型，将句子encode到潜在空间，一个解码器模型从潜在空间返回到文本表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建一个编码器模型，将句子encode到潜在空间\n",
    "encoder = Model(x, z_mean)\n",
    "#encoder.save('models/encoder32dim512hid30kvocab_loss29_val34.h5')\n",
    "\n",
    "#一个解码器模型从潜在空间返回到文本表示\n",
    "decoder_input = Input(shape=(latent_dim,))\n",
    "_h_decoded = decoder_h(repeated_context(decoder_input))\n",
    "_x_decoded_mean = decoder_mean(_h_decoded)\n",
    "_x_decoded_mean = Activation('softmax')(_x_decoded_mean)\n",
    "generator = Model(decoder_input, _x_decoded_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 在validation句子上测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad how can i self lion tripit\n",
      "pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad pad how can i self teach copywriting\n"
     ]
    }
   ],
   "source": [
    "index2word = {v: k for k, v in word_index.items()}\n",
    "index2word[0] = 'pad'\n",
    "\n",
    "#在validation句子上测试\n",
    "sent_idx = 30\n",
    "sent_encoded = encoder.predict(data_val[sent_idx:sent_idx+2,:])\n",
    "x_test_reconstructed = generator.predict(sent_encoded, batch_size = 1)\n",
    "reconstructed_indexes = np.apply_along_axis(np.argmax, 1, x_test_reconstructed[0])\n",
    "#np.apply_along_axis(np.max, 1, x_test_reconstructed[0])\n",
    "#np.max(np.apply_along_axis(np.max, 1, x_test_reconstructed[0]))\n",
    "word_list = list(np.vectorize(index2word.get)(reconstructed_indexes))\n",
    "print(' '.join(word_list))\n",
    "original_sent = list(np.vectorize(index2word.get)(data_val[sent_idx]))\n",
    "print(' '.join(original_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 句子处理和插值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#解析句子函数\n",
    "def sent_parse(sentence, mat_shape):\n",
    "    sequence = tokenizer.texts_to_sequences(sentence)\n",
    "    padded_sent = pad_sequences(sequence, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    return padded_sent#[padded_sent, sent_one_hot]\n",
    "\n",
    "# input: encode后的句子向量\n",
    "# output: 具有最高余弦相似性的数据集中的编码句子向量\n",
    "def find_similar_encoding(sent_vect):\n",
    "    all_cosine = []\n",
    "    for sent in sent_encoded:\n",
    "        result = 1 - spatial.distance.cosine(sent_vect, sent)\n",
    "        all_cosine.append(result)\n",
    "    data_array = np.array(all_cosine)\n",
    "    maximum = data_array.argsort()[-3:][::-1][1]\n",
    "    new_vec = sent_encoded[maximum]\n",
    "    return new_vec\n",
    "\n",
    "# input: 两个点，整数n\n",
    "# output:n个输入点之间的线上的等距点（包括）\n",
    "def shortest_homology(point_one, point_two, num):\n",
    "    dist_vec = point_two - point_one\n",
    "    sample = np.linspace(0, 1, num, endpoint = True)\n",
    "    hom_sample = []\n",
    "    for s in sample:\n",
    "        hom_sample.append(point_one + s * dist_vec)\n",
    "    return hom_sample\n",
    "\n",
    "# input:原始维度句子向量\n",
    "# output: 句子文本\n",
    "def print_latent_sentence(sent_vect):\n",
    "    sent_vect = np.reshape(sent_vect,[1,latent_dim])\n",
    "    sent_reconstructed = generator.predict(sent_vect)\n",
    "    sent_reconstructed = np.reshape(sent_reconstructed,[max_len,NB_WORDS])\n",
    "    reconstructed_indexes = np.apply_along_axis(np.argmax, 1, sent_reconstructed)\n",
    "    word_list = list(np.vectorize(index2word.get)(reconstructed_indexes))\n",
    "    w_list = [w for w in word_list if w not in ['pad']]\n",
    "    print(' '.join(w_list))\n",
    "    #print(word_list)\n",
    "     \n",
    "def new_sents_interp(sent1, sent2, n):\n",
    "    tok_sent1 = sent_parse(sent1, [MAX_SEQUENCE_LENGTH + 2])\n",
    "    tok_sent2 = sent_parse(sent2, [MAX_SEQUENCE_LENGTH + 2])\n",
    "    enc_sent1 = encoder.predict(tok_sent1, batch_size = 16)\n",
    "    enc_sent2 = encoder.predict(tok_sent2, batch_size = 16)\n",
    "    test_hom = shortest_homology(enc_sent1, enc_sent2, n)\n",
    "    for point in test_hom:\n",
    "        print_latent_sentence(point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 示例\n",
    "可以解析两个句子并在它们之间插入生成新句子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a unk can i find a bad best part\n",
      "why unk i where consultancy\n",
      "they unk can i available can restaurant can an part\n",
      "why unk i where consultancy\n",
      "-----------------\n",
      "a unk can i find a bad best part\n",
      "a unk can i find sahara cotton restaurant part\n",
      "unk can i available find restaurant restaurant through part\n",
      "they unk can i available can restaurant can through part\n",
      "they unk can i available can restaurant can an part\n"
     ]
    }
   ],
   "source": [
    "sentence1=['gogogo where can i find a bad restaurant endend']\n",
    "mysent = sent_parse(sentence1, [MAX_SEQUENCE_LENGTH + 2])\n",
    "mysent_encoded = encoder.predict(mysent, batch_size = 16)\n",
    "print_latent_sentence(mysent_encoded)\n",
    "print_latent_sentence(find_similar_encoding(mysent_encoded))\n",
    "\n",
    "sentence2=['gogogo where can i find an extremely good restaurant endend']\n",
    "mysent2 = sent_parse(sentence2, [MAX_SEQUENCE_LENGTH + 2])\n",
    "mysent_encoded2 = encoder.predict(mysent2, batch_size = 16)\n",
    "print_latent_sentence(mysent_encoded2)\n",
    "print_latent_sentence(find_similar_encoding(mysent_encoded2))\n",
    "print('-----------------')\n",
    "\n",
    "new_sents_interp(sentence1, sentence2, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 结尾\n",
    "另，\n",
    "结果还不完全令人满意，有很多句子有语法问题，并且在插值中多次生成相同的句子。\n",
    "改进：\n",
    " - 参数调整（可尝试更大的网络）\n",
    " - 更一般化的数据集（Quora句子都是问句）"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
