{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入需要的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Bidirectional, Dense, Embedding, Input, Lambda, LSTM, RepeatVector, TimeDistributed, Layer, Activation, Dropout\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers.advanced_activations import ELU\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from scipy import spatial\n",
    "from random import shuffle\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import codecs\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载目录和文档\n",
    "首先，我们将设置主目录和一些有关文本特征的变量。我们将序列长度设置为5-20，将词汇表中的最大单词数设置为270000(原270054)，我们将使用300维embeddings。最后，从txt加载文本。文本文件来源于新闻、网页、小说等，包含大约287万个句子(130万条符合长度要求)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1372687 texts in train.txt\n"
     ]
    }
   ],
   "source": [
    "BASE_DIR = './data/'\n",
    "TRAIN_DATA_FILE = BASE_DIR + 'train.txt'# 287+万条句子\n",
    "GLOVE_EMBEDDING = BASE_DIR + 'sgns.renmin.bigram-char'#单词->300维embedding\n",
    "VALIDATION_SPLIT = 0.2\n",
    "MIN_SEQUENCE_LENGTH = 5  #最小序列长度5\n",
    "MAX_SEQUENCE_LENGTH = 20 #最大序列长度20\n",
    "MAX_NB_WORDS = 270000\n",
    "EMBEDDING_DIM = 300 #embedding维度300\n",
    "\n",
    "texts = [] #通过列表来存储句子\n",
    "with codecs.open(TRAIN_DATA_FILE, encoding='utf-8') as f:\n",
    "    reader = f.readline()\n",
    "    while reader: #取出句子,存入texts\n",
    "        if (len(reader.split()) <= MAX_SEQUENCE_LENGTH) and (len(reader.split()) >= MIN_SEQUENCE_LENGTH):\n",
    "            texts.append(reader)\n",
    "        reader = f.readline()\n",
    "f.close()\n",
    "\n",
    "n_sents = len(texts)\n",
    "print('Found %s texts in train.txt' % n_sents) #训练用句子个数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文本预处理\n",
    "使用Keras的tokenizer和text_to_sequences函数预处理文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 270054 unique tokens\n",
      "Shape of data tensor: (1372687, 20)\n",
      "NB_WORDS: 270001\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(MAX_NB_WORDS+1, oov_token='unk') #Tokenizer是一个用于向量化文本，或将文本转换为序列（即单词在字典中的下标构成的列表，从1算起）的类\n",
    "tokenizer.fit_on_texts(texts)\n",
    "print('Found %s unique tokens' % len(tokenizer.word_index))\n",
    "\n",
    "## **关键步骤** 若不能正常工作，丢弃OOV_Token\n",
    "tokenizer.word_index = {e:i for e,i in tokenizer.word_index.items() if i <= MAX_NB_WORDS} # <= 从1开始\n",
    "#tokenizer.word_index[tokenizer.oov_token] = MAX_NB_WORDS + 1\n",
    "word_index = tokenizer.word_index #word到index的字典\n",
    "index2word = {v: k for k, v in word_index.items()} #index到word的字典\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(texts)#序列的列表，列表中每个序列对应于一段输入文本\n",
    "shuffle(sequences) #打乱句子\n",
    "data_1 = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH) #序列全部填充到25维，尾补0\n",
    "print('Shape of data tensor:', data_1.shape)\n",
    "NB_WORDS = (min(tokenizer.num_words, len(word_index))+1) #+1 for zero padding \n",
    "print('NB_WORDS:', NB_WORDS)\n",
    "\n",
    "data_val = data_1[1000000:1200000]\n",
    "data_train = data_1[:1000000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(word_index['unk'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "270001",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-54a2640198b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex2word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m270001\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 270001"
     ]
    }
   ],
   "source": [
    "print(index2word[270001])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embeddings\n",
    "使用预训练的中文 word embeddings(355776个)。创建一个矩阵，在词汇表中为每个单词对应一个embedding，然后我们将这个矩阵作为权重传递给我们模型的embedding layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 355776 word vectors.\n",
      "Null word embeddings: 1\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "unk_embedding = np.zeros(300)\n",
    "\n",
    "#取出word及其对应的embeddings，存入字典embeddings_index\n",
    "with codecs.open(GLOVE_EMBEDDING, encoding='utf-8') as f:\n",
    "    line = f.readline()\n",
    "    line = f.readline()\n",
    "    \n",
    "    while line: #取出句子,存入texts\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        unk_embedding = unk_embedding + coefs\n",
    "        embeddings_index[word] = coefs\n",
    "        line = f.readline()\n",
    "f.close()\n",
    "unk_embedding = unk_embedding / len(embeddings_index)\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "#print(unk_embedding)\n",
    "\n",
    "glove_embedding_matrix = np.zeros((NB_WORDS, EMBEDDING_DIM)) #申请0数组，(413501,300)\n",
    "for word, i in word_index.items():\n",
    "    if i < NB_WORDS+1: #+1 for 'unk' oov token \n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            glove_embedding_matrix[i] = embedding_vector\n",
    "        else:\n",
    "            # 在embeddings索引中找不到的单词，将是unk的embeddings\n",
    "            #print('i=',i)\n",
    "            glove_embedding_matrix[i] = unk_embedding\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(glove_embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAE 模型\n",
    "模型基于seq2seq架构，包含双向LSTM编码器和LSTM解码器。\n",
    "通过 RepeatVector（max_len）函数，将每个时间步的潜在表示作为输入提供给解码器decoder。为了避免标签的独热码表示，我们使用tf.contrib.seq2seq.sequence_loss函数，它只需要单词索引作为标签（与embedding矩阵的输入相同）并在内部计算最终的softmax（所以 模型以具有线性激活的dense层结束）。 \n",
    "可选地，“sequence_loss”允许使用采样的softmax，这有助于处理大型词汇表（例如，具有50k字词汇），但在此没有使用。这里使用的解码器与文中实现的解码器不同; 不是将context vector作为解码器的初始状态和预测的单词作为输入，而是在每个时间步处输入潜在表示z作为输入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ywj/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/ywj/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "(?, 40) (?, 40, 413501)\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 40)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 40, 300)      124050300   input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 512)          1140736     embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 64)           32832       bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 64)           32832       bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 64)           0           dense_1[0][0]                    \n",
      "                                                                 dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_1 (RepeatVector)  (None, 40, 64)       0           lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 40, 256)      328704      repeat_vector_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 40, 413501)   106269757   lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "custom_variational_layer_1 (Cus [(None, 40), (None,  0           input_1[0][0]                    \n",
      "                                                                 dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 231,855,161\n",
      "Trainable params: 107,804,861\n",
      "Non-trainable params: 124,050,300\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "max_len = MAX_SEQUENCE_LENGTH\n",
    "emb_dim = EMBEDDING_DIM\n",
    "latent_dim = 64\n",
    "intermediate_dim = 256\n",
    "epsilon_std = 1.0\n",
    "kl_weight = 0.01\n",
    "num_sampled=500\n",
    "act = ELU()\n",
    "\n",
    "\n",
    "x = Input(shape=(max_len,)) #输入是按批量的40维向量(句子)\n",
    "x_embed = Embedding(NB_WORDS, emb_dim, weights=[glove_embedding_matrix], input_length=max_len, trainable=False)(x)\n",
    "h = Bidirectional(LSTM(intermediate_dim, return_sequences=False, recurrent_dropout=0.2), merge_mode='concat')(x_embed)\n",
    "#h = Bidirectional(LSTM(intermediate_dim, return_sequences=False), merge_mode='concat')(h)\n",
    "#h = Dropout(0.2)(h)\n",
    "#h = Dense(intermediate_dim, activation='linear')(h)\n",
    "#h = act(h)\n",
    "#h = Dropout(0.2)(h)\n",
    "z_mean = Dense(latent_dim)(h)\n",
    "z_log_var = Dense(latent_dim)(h)\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = K.random_normal(shape=(batch_size, latent_dim), mean=0.,\n",
    "                              stddev=epsilon_std)\n",
    "    return z_mean + K.exp(z_log_var / 2) * epsilon\n",
    "\n",
    "# note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "# 分别实例化这些层，以便以后重用\n",
    "repeated_context = RepeatVector(max_len)\n",
    "decoder_h = LSTM(intermediate_dim, return_sequences=True, recurrent_dropout=0.2)\n",
    "decoder_mean = Dense(NB_WORDS, activation='linear')#softmax is applied in the seq2seqloss by tf #TimeDistributed()\n",
    "h_decoded = decoder_h(repeated_context(z))\n",
    "x_decoded_mean = decoder_mean(h_decoded)\n",
    "\n",
    "\n",
    "# placeholder loss\n",
    "def zero_loss(y_true, y_pred):\n",
    "    return K.zeros_like(y_pred)\n",
    "\n",
    "'''\n",
    "#Sampled softmax\n",
    "logits = tf.constant(np.random.randn(batch_size, max_len, NB_WORDS), tf.float32)\n",
    "targets = tf.constant(np.random.randint(NB_WORDS, size=(batch_size, max_len)), tf.int32)\n",
    "proj_w = tf.constant(np.random.randn(NB_WORDS, NB_WORDS), tf.float32)\n",
    "proj_b = tf.constant(np.zeros(NB_WORDS), tf.float32)\n",
    "\n",
    "def _sampled_loss(labels, logits):\n",
    "    labels = tf.cast(labels, tf.int64)\n",
    "    labels = tf.reshape(labels, [-1, 1])\n",
    "    logits = tf.cast(logits, tf.float32)\n",
    "    return tf.cast(\n",
    "                    tf.nn.sampled_softmax_loss(\n",
    "                        proj_w,\n",
    "                        proj_b,\n",
    "                        labels,\n",
    "                        logits,\n",
    "                        num_sampled=num_sampled,\n",
    "                        num_classes=NB_WORDS),\n",
    "                    tf.float32)\n",
    "softmax_loss_f = _sampled_loss\n",
    "'''\n",
    "\n",
    "\n",
    "# 用于计算VAE损失的自定义层\n",
    "class CustomVariationalLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.is_placeholder = True\n",
    "        super(CustomVariationalLayer, self).__init__(**kwargs)\n",
    "        self.target_weights = tf.constant(np.ones((batch_size, max_len)), tf.float32)\n",
    "\n",
    "    def vae_loss(self, x, x_decoded_mean):\n",
    "        #xent_loss = K.sum(metrics.categorical_crossentropy(x, x_decoded_mean), axis=-1)\n",
    "        labels = tf.cast(x, tf.int32)\n",
    "        xent_loss = K.sum(tf.contrib.seq2seq.sequence_loss(x_decoded_mean, labels, \n",
    "                                                     weights=self.target_weights,\n",
    "                                                     average_across_timesteps=False,\n",
    "                                                     average_across_batch=False), axis=-1)\n",
    "                                                     #softmax_loss_function=softmax_loss_f), axis=-1)#,\n",
    "        kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "        xent_loss = K.mean(xent_loss)\n",
    "        kl_loss = K.mean(kl_loss)\n",
    "        return K.mean(xent_loss + kl_weight * kl_loss)\n",
    "    \n",
    "    #编写一个call方法，来实现自定义层\n",
    "    def call(self, inputs):\n",
    "        x = inputs[0]\n",
    "        x_decoded_mean = inputs[1]\n",
    "        print(x.shape, x_decoded_mean.shape)\n",
    "        loss = self.vae_loss(x, x_decoded_mean)\n",
    "        self.add_loss(loss, inputs=inputs)\n",
    "        # we don't use this output, but it has to have the correct shape:\n",
    "        return K.ones_like(x)\n",
    "    \n",
    "def kl_loss(x, x_decoded_mean):\n",
    "    kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "    kl_loss = kl_weight * kl_loss\n",
    "    return kl_loss\n",
    "\n",
    "loss_layer = CustomVariationalLayer()([x, x_decoded_mean])\n",
    "vae = Model(x, [loss_layer])\n",
    "opt = Adam(lr=0.01) \n",
    "vae.compile(optimizer='adam', loss=[zero_loss], metrics=[kl_loss])\n",
    "vae.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型训练\n",
    "通过keras.fit()训练100epochs。对于验证数据，传递相同的数组两次，因为此模型的输入和标签相同。\n",
    "如果不使用“tf.contrib.seq2seq.sequence_loss”（或其他类似的函数），\n",
    "将必须作为标签传递单词的one-hot码高维度序列(batch_size，seq_len，vocab_size)消耗大量内存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ywj/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 2700000 samples, validate on 70000 samples\n",
      "Epoch 1/100\n",
      "    100/2700000 [..............................] - ETA: 649:32:59 - loss: 517.2673 - kl_loss: 0.0052"
     ]
    }
   ],
   "source": [
    "def create_model_checkpoint(dir, model_name):\n",
    "    filepath = dir + '/' + model_name + \".h5\" \n",
    "    directory = os.path.dirname(filepath)\n",
    "    try:\n",
    "        os.stat(directory)\n",
    "    except:\n",
    "        os.mkdir(directory)\n",
    "    checkpointer = ModelCheckpoint(filepath=filepath, verbose=1, save_best_only=True)\n",
    "    return checkpointer\n",
    "\n",
    "checkpointer = create_model_checkpoint('models', 'vae_seq2seq_test_very_high_std')\n",
    "\n",
    "\n",
    "\n",
    "vae.fit(data_train, data_train,\n",
    "     shuffle=True,\n",
    "     epochs=100,\n",
    "     batch_size=batch_size,\n",
    "     validation_data=(data_val, data_val), callbacks=[checkpointer])\n",
    "\n",
    "#print(K.eval(vae.optimizer.lr))\n",
    "#K.set_value(vae.optimizer.lr, 0.01)\n",
    "\n",
    "vae.save('models/vae_lstm.h5')\n",
    "#vae.load_weights('models/vae_lstm.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
